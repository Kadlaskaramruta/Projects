{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Fast Food Store\" dataset includes images of 5 famous fast food stores and their food images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.set_random_seed(2019) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formation of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2), \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(), \n",
    "\n",
    "    tf.keras.layers.Dense(200, activation='relu'), \n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')  \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here we can see Summary of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 200)               3699400   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 3,743,589\n",
      "Trainable params: 3,743,589\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specifying the Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop,SGD\n",
    "\n",
    "sgd=SGD(lr=0.1)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting the Data Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1465 images belonging to 5 classes.\n",
      "Found 893 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "bs=15         #Setting batch size\n",
    "    \n",
    "train_dir = \"D:/Data Science/Image Datasets/FastFood/train/\"  #Setting training directory\n",
    "validation_dir = \"D:/Data Science/Image Datasets/FastFood/test/\"  #Setting testing directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255.\n",
    "train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
    "test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
    "\n",
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    batch_size=bs,\n",
    "                                                    class_mode='categorical',target_size=(150,150))     \n",
    "\n",
    "# Flow validation images in batches of 20 using test_datagen generator\n",
    "\n",
    "validation_generator =  test_datagen.flow_from_directory(validation_dir,\n",
    "                                                         batch_size=bs,\n",
    "                                                         class_mode  = 'categorical',target_size=(150,150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "13/13 [==============================] - 3s 227ms/step - loss: 0.6874 - acc: 0.7026 - val_loss: 1.3812 - val_acc: 0.5267\n",
      "Epoch 2/25\n",
      "13/13 [==============================] - 3s 258ms/step - loss: 1.2287 - acc: 0.5128 - val_loss: 1.4772 - val_acc: 0.4133\n",
      "Epoch 3/25\n",
      "13/13 [==============================] - 3s 221ms/step - loss: 1.0629 - acc: 0.6158 - val_loss: 1.3681 - val_acc: 0.5267\n",
      "Epoch 4/25\n",
      "13/13 [==============================] - 3s 219ms/step - loss: 0.9627 - acc: 0.6564 - val_loss: 1.5153 - val_acc: 0.4733\n",
      "Epoch 5/25\n",
      "13/13 [==============================] - 3s 217ms/step - loss: 0.9995 - acc: 0.6051 - val_loss: 1.5107 - val_acc: 0.4000\n",
      "Epoch 6/25\n",
      "13/13 [==============================] - 3s 216ms/step - loss: 0.7536 - acc: 0.7263 - val_loss: 1.5145 - val_acc: 0.4533\n",
      "Epoch 7/25\n",
      "13/13 [==============================] - 3s 217ms/step - loss: 0.7472 - acc: 0.7263 - val_loss: 1.3765 - val_acc: 0.5267\n",
      "Epoch 8/25\n",
      "13/13 [==============================] - 3s 219ms/step - loss: 0.7254 - acc: 0.7231 - val_loss: 1.2683 - val_acc: 0.5267\n",
      "Epoch 9/25\n",
      "13/13 [==============================] - 3s 236ms/step - loss: 0.7256 - acc: 0.7385 - val_loss: 1.0861 - val_acc: 0.5600\n",
      "Epoch 10/25\n",
      "13/13 [==============================] - 3s 221ms/step - loss: 0.7026 - acc: 0.7385 - val_loss: 1.3278 - val_acc: 0.5467\n",
      "Epoch 11/25\n",
      "13/13 [==============================] - 3s 234ms/step - loss: 0.6769 - acc: 0.7487 - val_loss: 1.2657 - val_acc: 0.5600\n",
      "Epoch 12/25\n",
      "13/13 [==============================] - 3s 219ms/step - loss: 0.5774 - acc: 0.8105 - val_loss: 1.3833 - val_acc: 0.5667\n",
      "Epoch 13/25\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 0.5102 - acc: 0.8263 - val_loss: 1.4002 - val_acc: 0.6000\n",
      "Epoch 14/25\n",
      "13/13 [==============================] - 3s 209ms/step - loss: 0.6449 - acc: 0.7590 - val_loss: 1.2372 - val_acc: 0.5667\n",
      "Epoch 15/25\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.5837 - acc: 0.7846 - val_loss: 1.7191 - val_acc: 0.4067\n",
      "Epoch 16/25\n",
      "13/13 [==============================] - 3s 233ms/step - loss: 0.5015 - acc: 0.8256 - val_loss: 1.1345 - val_acc: 0.6000\n",
      "Epoch 17/25\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.4039 - acc: 0.8474 - val_loss: 1.4010 - val_acc: 0.5533\n",
      "Epoch 18/25\n",
      "13/13 [==============================] - 3s 231ms/step - loss: 0.3758 - acc: 0.8872 - val_loss: 1.5688 - val_acc: 0.5667\n",
      "Epoch 19/25\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.5286 - acc: 0.8256 - val_loss: 1.3874 - val_acc: 0.5733\n",
      "Epoch 20/25\n",
      "13/13 [==============================] - 3s 235ms/step - loss: 0.3839 - acc: 0.8513 - val_loss: 1.1925 - val_acc: 0.6267\n",
      "Epoch 21/25\n",
      "13/13 [==============================] - 3s 207ms/step - loss: 0.4037 - acc: 0.8718 - val_loss: 1.0802 - val_acc: 0.6267\n",
      "Epoch 22/25\n",
      "13/13 [==============================] - 3s 267ms/step - loss: 0.2842 - acc: 0.9179 - val_loss: 1.3288 - val_acc: 0.5867\n",
      "Epoch 23/25\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 0.2778 - acc: 0.8974 - val_loss: 1.4862 - val_acc: 0.6067\n",
      "Epoch 24/25\n",
      "13/13 [==============================] - 3s 267ms/step - loss: 0.4291 - acc: 0.8667 - val_loss: 1.2350 - val_acc: 0.6467\n",
      "Epoch 25/25\n",
      "13/13 [==============================] - 3s 217ms/step - loss: 0.3648 - acc: 0.8821 - val_loss: 1.7589 - val_acc: 0.5600\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    steps_per_epoch=200 // bs,\n",
    "                    epochs=25,\n",
    "                    validation_steps=150 // bs,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hence, we see that sufficient accuracy has been met . However, anyone can run this model by increasing number of epoches or any other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Final_model.h5')      #Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
